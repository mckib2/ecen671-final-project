\documentclass[journal]{IEEEtran}
\usepackage{cite}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}

\graphicspath{ {figures/} }

\title{Optimizing Compressed Sensing Sensing Matrices}
\author{Nicholas McKibben and Connor Anderson}

\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

An abstract

\end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Compressed Sensing (CS) is a technique for reconstructing a signal that has been
highly undersampled. It is impossible to recover an undersampled signal in 
general: there is no way to know what the missing information was. However, when
the original signal meets certain constraints, it becomes possible to recover it
from its undersampled representation with a high (or even perfect) degree of
accuracy. For example, if the highest-frequency component $f$ of a signal is
known, the signal can be perfectly recovered if it is sampled uniformly with a 
sampling rate of $2f$ (the Nyquist sampling theorem). Compressed sensing is a
method for recovering signals that have been undersampled to an even greater
degree. In order for a signal to be recoverable through compressed sensing, it
must meet two constraints. First, it must have a sparse representation in some
transform domain. Second, the sampling must be done incoherently so that the
resulting aliasing appears like a relatively low level of noise in the transform
domain. Luckily, many signals of interest can be shown to have sparse
representations is some domain, such as the Wavelet Transform or the Discrete
Cosine Transform. If those signals are then sampled in the right way, they can
be represented by far fewer data points but recovered almost exactly.

In this work, we explore compressed sensing for recovering highly undersampled
images. We outline the method and mathematics involved, as well as some recent
developments in more effective sensing matrices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{THEORY OF COMPRESSED SENSING}

The notion of sparse representation and reconstruction is actually a well-studied problem within statistics, first described by Santosa and Symes in 1986 \cite{santosa}.  But the ramifications of the robustness of the reconstructions provided by this theory have only started to be realized in the past decade, with an emphasis on sparse sensing instead of retroactive application to already complete datasets, hence the name ``compressed sensing'' \cite{donoho}.

This reemergence of CS grew out of the pioneering work of Cand\`es, Romberg, Tao, and Donoho, who demonstrated that \emph{n}-dimensional signals with sparse representations can be exactly reconstructed from a set of linear, non-adaptive measurements in a wide variety of cases \cite{csbook,baraniuk,candes,donoho}.

CS differs from classical sampling theory in a few important ways.  First, classical sampling theory typically considers infinite dimensional, continuous-time signals.  CS restricts itself to finite, \emph{n}-dimensional signals.  Secondly, CS formulations usually do not sample the signal at specific points in time as is done classically.  Rather, measurements are acquired in the form of inner products between the signal and more general test functions (usually including some random component) \cite{csbook}.  Lastly, Nyquist-Shannon relies on sinc interpolation, whereas CS signal recovery typically uses highly non-linear methods \cite{oppenheimdigital,csbook}.

%Intuitively, we can understand CS as inducing a pseudo-noisefloor by randomly undersampling the signal of interest.  While uniform undersampling leads to coherent artifacting, random undersampling leads to incoherent aliasing in the measurement domain.  In the sparse domain, the incoherent aliasing leads to a noise floor with identifiably high sparse coefficients.  An iterative method based on thresholding, recovering the strong components, calculating the interference caused by them, and subtracting the calculated interference from the noisefloor while retroactively enforcing data consistency can be used to understand the reconstruction process \cite{lustig}.

Formally, CS can be understood as a method of finding an exact (or close) solution to the highly underdetermined system $Ax = b$, with $A$ being an $m \times n$ matrix such that $m \ll n$.  To find a unique solution, the system is regularized by imposing sparsity using the $\ell_0$ pseudo-norm in the constrained, highly non-convex optimization problem $$ \min_{\forall x \in \mathbb{R}^n}  ||x||_0 \mbox{ subject to } Ax = b $$ as in \cite{convexoptimization,sparseapprox}.  The $\ell_0$ quasi-norm is known to be NP-hard and is shown to be approximately equivalent in most cases to the more tractable convex optimization problem using the $\ell_1$ norm \cite{convexoptimization}.  Many CS methods exhibit denoising effects and are described by the modified constrained optimization problem $$ \min_{\forall x \in \mathbb{R}^n} ||x||_1 \mbox{ subject to } ||Ax - b||_2 \leq \sigma, $$ taking into account the standard deviation, $\sigma$, of the noise \cite{basispursuit}.

\section{MATH}

Here we describe the math behind CS. We have a set of signals $\{x_j \in \mathbb{R}^n\}$ that we are interested in measuring. We assume that $x_j$ has a sparse representation in some transform domain, such as the Discrete Cosine Transform (DCT). If we have a dictionary of vectors $D \in \mathbb{R}^{n\times k}$ that spans the transform domain, then we can represent $x_j$ as a linear combination of $D$: $x_j=D\alpha_j$, where $||\alpha_j||_0 \ll n$. This simply says that we can represent $x_j$ as a linear combination of a small number of the columns of $D$. If this is the case, then there is a lot of redundant information in our signal $x_j$, and it can be accurately (or perfectly) represented in a much-lower dimensional vector space $\mathbb{R}^p$ as $$y_j=Px_j=PD\alpha_j$$ where $P \in \mathbb{R}^{p\times n}$ with $p \ll n$ is a projection matrix from $\mathbb{R}^n$ to the lower-dimensional space $\mathbb{R}^p$. We can then recover $x_j$ from $y_j$ by finding the sparsest $\alpha$ that satisfies $y_j=PD\alpha_j$.

What this tells us is that it should be possible recover $x_j$ from very few samples, provided we sample in an appropriate way. It has been shown that sampling randomly generally works well because it increases the incoherence of something or other... \cite{somehinggoeshere}.

\section{SENSING MATRICES}

Successful CS reconstruction is achieved if certain properties of the sampling operator can be guaranteed (e.g., the Restricted Isometry Property, mutual coherence minimization, or the nullspace property \cite{tcs}).  These properties ensure sufficient conditions on the equivalent sensing matrix\footnote{The equivalent sensing matrix is the product of the sensing matrix and the sparsifying basis} for accurate signal recovery. Random matrices (i.e., Gaussian or Bernoulli) are known to satisfy these properties for any given sparsifying basis and are used widely sensing matrices in many CS applications.

\section{RESULTS}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CONCLUSION}

A conclusion


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{IEEEtran}
\bibliography{project}{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
